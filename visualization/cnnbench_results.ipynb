{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1609338514654,
     "user": {
      "displayName": "Shikhar Tuli",
      "photoUrl": "",
      "userId": "08476644503706754908"
     },
     "user_tz": -330
    },
    "id": "oFhFRmck7NzM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Useful constants\n",
    "INPUT = 'input'\n",
    "OUTPUT = 'output'\n",
    "CONV3X3 = 'conv3x3-bn-relu'\n",
    "CONV1X1 = 'conv1x1-bn-relu'\n",
    "MAXPOOL3X3 = 'maxpool3x3'\n",
    "NUM_VERTICES = 7\n",
    "MAX_EDGES = 9\n",
    "EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2   # Upper triangular matrix\n",
    "OP_SPOTS = NUM_VERTICES - 2   # Input/output vertices are fixed\n",
    "ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "ALLOWED_EDGES = [0, 1]   # Binary adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.io.gfile.GFile('../results/vertices_2/evaluation/04/043721b9c7fe8c5fad811d47d83132ec/repeat_1/results.json') as f:\n",
    "    result = json.load(f)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297137,
     "status": "ok",
     "timestamp": 1609338813122,
     "user": {
      "displayName": "Shikhar Tuli",
      "photoUrl": "",
      "userId": "08476644503706754908"
     },
     "user_tz": -330
    },
    "id": "i9SOVEvxrwwa",
    "outputId": "2c0f7df3-6f31-406e-a60f-a915960c53bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset\n",
      "Completed: 100.00%"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "max_count = 423624 * 3\n",
    "\n",
    "adjacency_list = []\n",
    "operations_list = []\n",
    "\n",
    "print('Generating dataset')\n",
    "for serialized_row in tf.python_io.tf_record_iterator(dataset_file):\n",
    "\n",
    "  count += 1\n",
    "  print('\\rCompleted: %0.2f%%' % (count/(max_count)*100), end='')\n",
    "\n",
    "  # Take only unique adjacency matrices\n",
    "  if count % 3 != 0: continue\n",
    "\n",
    "  # Parse the data from the data file.\n",
    "  module_hash, epochs, raw_adjacency, raw_operations, raw_metrics = (\n",
    "      json.loads(serialized_row.decode('utf-8')))\n",
    "\n",
    "  dim = int(np.sqrt(len(raw_adjacency)))\n",
    "  adjacency = np.array([int(e) for e in list(raw_adjacency)], dtype=np.int8)\n",
    "  adjacency = np.reshape(adjacency, (dim, dim))\n",
    "  operations = raw_operations.split(',')\n",
    "  metrics = model_metrics_pb2.ModelMetrics.FromString(\n",
    "      base64.b64decode(raw_metrics))\n",
    "  \n",
    "  adjacency_list.append(adjacency)\n",
    "  operations_list.append(operations)\n",
    "\n",
    "  # Evaluation statistics at the end of training\n",
    "  '''\n",
    "  final_evaluation = metrics.evaluation_data[2]\n",
    "  training_time = final_evaluation.training_time\n",
    "  train_accuracy = final_evaluation.train_accuracy\n",
    "  validation_accuracy = (\n",
    "      final_evaluation.validation_accuracy)\n",
    "  test_accuracy = final_evaluation.test_accuracy\n",
    "  trainable_params = metrics.trainable_parameters\n",
    "\n",
    "  print(f'Module {count//3+1}.{count%3+1} \\nAdjacency matrix: \\n{adjacency} \\nOperations: {operations} \\nTrainable parameters: {trainable_params}')\n",
    "  print(f'Train Accuracy: {train_accuracy} \\nValidation Accuracy: {validation_accuracy} \\nTest Accuracy: {test_accuracy}\\n')\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82548,
     "status": "ok",
     "timestamp": 1609339514328,
     "user": {
      "displayName": "Shikhar Tuli",
      "photoUrl": "",
      "userId": "08476644503706754908"
     },
     "user_tz": -330
    },
    "id": "PQy0zMtsSajm",
    "outputId": "32279086-9469-4863-a6d0-36539eb35c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting dataset\n",
      "Completed: 100.00%"
     ]
    }
   ],
   "source": [
    "ordered_operations = [OUTPUT, INPUT, MAXPOOL3X3, CONV1X1, CONV3X3]\n",
    "\n",
    "weighted_adjacency_list = copy.deepcopy(adjacency_list)\n",
    "compressed_weighted_list = []\n",
    "\n",
    "count = 0\n",
    "\n",
    "print('Augmenting dataset')\n",
    "for i in range(len(weighted_adjacency_list)):\n",
    "\n",
    "  count += 1\n",
    "  print('\\rCompleted: %0.2f%%' % (count/(max_count/3)*100), end='')\n",
    "\n",
    "  vertices = np.shape(weighted_adjacency_list[i])[0]\n",
    "\n",
    "  for v in range(vertices):\n",
    "    weighted_adjacency_list[i][v, :] *= (ordered_operations.index(operations_list[i][v]))\n",
    "\n",
    "  compressed_weighted_list.append(compress(weighted_adjacency_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1609339530939,
     "user": {
      "displayName": "Shikhar Tuli",
      "photoUrl": "",
      "userId": "08476644503706754908"
     },
     "user_tz": -330
    },
    "id": "YhAs3K_9FBs5",
    "outputId": "fd69ec42-422b-430f-d475-d0e7348542e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0]]\n",
      "['input', 'conv3x3-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'maxpool3x3', 'maxpool3x3', 'output']\n",
      "[[0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 4]\n",
      " [0 0 0 4 0 0 0]\n",
      " [0 0 0 0 3 0 3]\n",
      " [0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(adjacency_list[n])\n",
    "print(operations_list[n])\n",
    "print(weighted_adjacency_list[n])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nasbench_var.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google-research/nasbench/blob/master/NASBench.ipynb",
     "timestamp": 1608040131043
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
